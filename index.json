[{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/4-eventparticipated/4.1-event1/","title":"Event 1: AI-Driven Development Life Cycle: Reimagining Software Engineering","tags":[],"description":"","content":"AI-Driven Development Life Cycle: Reimagining Software Engineering Speakers Toan Huynh\nCovering the AI-Driven Development Lifecycle overview Amazon Q Developer demo My Nguyen\nCovering the Kiro demo Key Takeaways Paradigm Shift: Understanding the revolutionary shift from traditional software engineering to AI-Driven Development.\nFull SDLC Integration: Learning how to integrate Generative AI into the entire Software Development Lifecycle (SDLC)‚Äîfrom architecture, planning, and creation to testing, deployment, and maintenance.\nStrategic Automation: Strategies for automating undifferentiated heavy lifting to increase productivity and allow focus on high-value, creative tasks.\nAmazon Q Developer Capabilities: Deep dive into how Amazon Q Developer assists developers in learning, planning, and managing applications.\nKiro Utility: Exploring the features and practical applications of Kiro in the development workflow.\nSkills Acquired Amazon Q Developer Utilization: Practical skills in using Amazon Q Developer to assist with coding and development tasks.\nNext-Gen Tooling: Familiarity with Kiro and its role in modern software engineering.\nAI-Enhanced Productivity: Techniques for leveraging GenAI to accelerate development cycles and reduce manual overhead.\nSecure Deployment: Understanding how to securely plan, create, and manage applications within an AI-integrated environment.\nModern SDLC Management: Ability to apply AI tools effectively across different stages of software architecture and maintenance.\nEvent Photos "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/4-eventparticipated/4.2-event2/","title":"Event 1: AI/ML/GenAI on AWS Workshop","tags":[],"description":"","content":"AI/ML/GenAI on AWS üìÖ Date: Saturday, November 15, 2025\nüï£ Time: 8:30 AM ‚Äì 12:00 PM\nüìç Location: AWS Vietnam Office\nEvent Overview This hands-on workshop provided comprehensive insights into AWS\u0026rsquo;s AI/ML services and Generative AI capabilities. The event focused on practical demonstrations and real-world applications of machine learning and generative AI technologies on AWS.\nEvent Agenda 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 9:00 ‚Äì 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker ‚Äì End-to-end ML platform\nData preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough 10:30 ‚Äì 10:45 AM | Coffee Break Networking opportunity with fellow participants and AWS experts\n10:45 AM ‚Äì 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan ‚Äì comparison \u0026amp; selection guide\nPrompt Engineering:\nTechniques and best practices Chain-of-Thought reasoning Few-shot learning Retrieval-Augmented Generation (RAG):\nArchitecture overview Knowledge Base integration Bedrock Agents:\nMulti-step workflows Tool integrations Guardrails:\nSafety measures Content filtering Live Demo: Building a Generative AI chatbot using Bedrock\nKey Takeaways Gained hands-on experience with Amazon SageMaker for end-to-end ML workflows Understood the capabilities and use cases of different foundation models available on Amazon Bedrock Learned practical prompt engineering techniques for better GenAI outputs Explored RAG architecture for building knowledge-based AI applications Discovered how to implement safety guardrails in GenAI applications Networked with AWS professionals and fellow AI/ML enthusiasts ML Lifecycle: Understanding the complete ML development lifecycle from data preparation to deployment SageMaker ecosystem: Mastering SageMaker components and usage MLOps practices: Applying DevOps for Machine Learning Foundation Model selection: Choosing the right model for each use case Prompt Engineering: Skills for writing effective prompts RAG architecture: Understanding and applying RAG to enhance LLMs with proprietary data AI Safety: Understanding the importance of guardrails in GenAI applications Skills Acquired Amazon SageMaker Studio navigation and usage Foundation model selection and comparison Prompt engineering best practices RAG implementation concepts Bedrock Agents configuration AI safety and content filtering strategies Event Photos "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Artificial Intelligence Prompting for precision with Stability AI Image Services in Amazon Bedrock by Suleman Patel, Isha Dua, Fabio Branco, and Maxfield Hulker on 18 SEP 2025 in Amazon Bedrock, Amazon SageMaker, Announcements, Artificial Intelligence, Foundation models, Generative AI, Launch Permalink Comments Share\nAmazon Bedrock now offers Stability AI Image Services: 9 tools that improve how businesses create and modify images. The technology extends Stable Diffusion and Stable Image models to give you precise control over image creation and editing. Clear prompts are critical‚Äîthey provide art direction to the AI system. Strong prompts control specific elements like tone, texture, lighting, and composition to create the desired visual outcomes. This capability serves professional needs across product photography, concept, and marketing campaigns.\nIn this post, we expand on the post Understanding prompt engineering: Unlock the creative potential of Stability AI models on AWS. We show how to effectively use advanced prompting techniques to maximize image generation quality and precision for enterprise application using Stability AI Image Services in Amazon Bedrock.\nSolution overview Stability AI Image Services are available as APIs in Amazon Bedrock, featuring capabilities such as, in-painting, style transfer, recoloring, background removal, object removal, style guide, and much more.\nIn the following sections, we first discuss prompt structure for maximum control of image generation, then we provide advanced techniques of prompting for stylistic guidance. Code samples can be found in the following GitHub repository.\nPrerequisites To get started with Stability AI Image Services in Amazon Bedrock, follow the instructions in Getting started with the API to complete the following prerequisites:\nSet up your AWS account. Acquire credentials to grant programmatic access. Attach the Amazon Bedrock permission to an AWS Identity and Access Management (IAM) user or role. Request access to the Amazon Bedrock models. Structure prompts that maximize control To maximize the granular capabilities of Stability AI Image Services in Amazon Bedrock, you must construct prompts that enable fine-grained control.\nThis section outlines best practices for building effective prompts that produce the desired output. We demonstrate how prompt structure affects results and why more structured prompts typically yield more consistent and controllable outcomes.\nChoose the right prompt type for your use case Selecting the right prompt format helps the model better understand your intent. Three primary prompt formats deliver different levels of control and readability:\nNatural language maximizes readability and is best for general usage Tag-based formats enable precise structural control and are ideal for technical application Hybrid formats combine natural language and the structural elements of tags to provide even more control The following table provides examples of these three common ways to phrase your prompts. Each prompt format has its strengths depending on your goal or the interface you‚Äôre using.\nPrompt type Prompt example Generated image using Stable Image Ultra in Amazon Bedrock Description and use case Basic Prompt (Natural Language) ‚ÄúA clean product photo of a perfume bottle on a marble countertop‚Äù This is readable and intuitive. Great for exploration, conversational tools, and some model types. Stable Diffusion 3.5 responds best to this style. Tag-Based Prompt ‚Äúperfume bottle, marble surface, soft light, high quality, product photo‚Äù Used in many generation UIs or with models trained on datasets like LAION or Danbooru. Compact and good for stacking details. Hybrid Prompt ‚Äúperfume bottle on marble counter, soft studio lighting, sharp focus, f/2.8lens‚Äù Best of both worlds. Add emphasis with weighting syntax to influence the model‚Äôs priorities. Build modular prompts Modular prompting enhances AI image generation effectiveness. This approach divides prompts into distinct components, each specifying what to draw and how it should appear. Modular structures provide several benefits: they help prevent conflicting or confusing instructions, allow for precise output control, and simplify prompt debugging. By isolating individual elements, you can quickly identify and adjust effective or ineffective parts of your prompts. This method ultimately leads to more refined and targeted AI-generated images.\nThe following table provides examples of modular prompt modules. Experiment with different prompt sequences for your desired outcome; for example, placing the style before the subject will give it a more visual weight.\nModule Example Description Prefix ‚Äúfashion editorial portrait of‚Äù Sets the tone and intent for a high-fashion styled portrait Subject ‚Äúa woman with medium-brown skin and short coiled hair‚Äù Gives the model‚Äôs look and surface detail to help guide facial features Modifiers ‚Äúwearing an asymmetrical black mesh top, metallic jewelry‚Äù Adds stylized clothing and accessories for visual interest Action ‚Äúseated with her shoulders angled, eyes locked on camera, one arm lifted‚Äù Describes body language and pose to give dynamic composition Environment ‚Äúbathed in intersecting beams of hard directional light through window slats‚Äù Adds context for dramatic light play and atmosphere Style ‚Äúhigh-contrast chiaroscuro lighting, sculptural and abstract‚Äù Informs the aesthetic and mood (shadow-driven, moody, architectural) Camera/Lighting ‚Äúshot on 85mm, studio setup, layered shadows and light falling across face and body‚Äù Adds technical precision and helps control realism and fidelity The following example illustrates how to use a modular prompt to generate the desired output.\nModular Prompt Generated Image Using Stable Image Ultra in Amazon Bedrock ‚Äúfashion editorial portrait of a woman with medium-brown skin and short coiled hair, wearing an asymmetrical black mesh top and metallic jewelry, seated with shoulders angled and one arm lifted, eyes locked on camera, bathed in intersecting beams of hard directional light through window slats, layered shadows and highlights sculpting her face and body, high-contrast chiaroscuro lighting, abstract and bold, shot on 85mm in studio‚Äù Use negative prompts for polished output prompts improve AI output quality by removing specific visual elements. Explicitly defining what not to include in the prompt guides the model‚Äôs output, typically leading to professional outputs. Negative prompts act like a retoucher‚Äôs cNegativehecklist used to address aspects of an image to enhance quality and appeal. For example, ‚ÄúNo weird hands. No blurry corners. No cartoon filters. Definitely no watermarks.‚Äù Negative prompts result in clean, confident, compositions, free of distracting element and distortions.\nThe following table provides examples of additional tokens that can be used in negative prompts.\nArtifact Type Tokens to Use Low quality or noise blurry, lowres, jpeg artifacts, noisy Anatomy or model issues deformed, extra limbs, bad hands, missing fingers Style clashes cartoon, illustration, anime, painting Technical errors watermark, text, signature, overexposed General cleanup ugly, poorly drawn, distortion, worst quality The following example illustrates how a well-structured negative prompt can enhance photorealism.\nWithout Negative Prompt Prompt ‚Äú(medium full shot) of (charming office cubicle) made of glass material, multiple colors, modern style, space-saving, upholstered seat, patina, gold trim, located in a modern garden, with sleek furniture, stylish decor, bright lighting, comfortable seating, Masterpiece, best quality, raw photo, realistic, very aesthetic, dark ‚Äú With Negative Prompt Prompt ‚Äú(medium full shot) of (charming office cubicle) made of glass material, multiple colors, modern style, space-saving, upholstered seat, patina, gold trim, located in a modern garden, with sleek furniture, stylish decor, bright lighting, comfortable seating, Masterpiece, best quality, raw photo, realistic, very aesthetic, dark‚Äù Negative Prompt ‚Äúcartoon, 3d render, cgi, oversaturated, smooth plastic textures, unreal lighting, artificial, matte surface, painterly, dreamy, glossy finish, digital art, low detail background‚Äù Emphasize or suppress elements with prompt weighting Prompt weighting controls the influence of individual elements in AI image generation. These numerical weights prioritize specific prompt components over others. For example, to emphasize the character over the background, you can apply a 1.8 weight to ‚Äúcharacter‚Äù (character: 1.8) and 1.1 to ‚Äúbackground‚Äù (background: 1.1), which makes sure the model prioritizes character detail while maintaining environmental context. This targeted emphasis produces more precise outputs by minimizing competition between prompt elements and clarifying the model‚Äôs priorities.\nThe syntax for prompt weights is (\u0026lt;term\u0026gt;:\u0026lt;weight\u0026gt;). You can also use a shorthand such as ((\u0026lt;term\u0026gt;)), where the number of parentheses represent the weight. Values between 0.0‚Äì1.0 deemphasize the term, and values between 1.1‚Äì2.0 emphasize the term.For example:\n(term:1.2): Emphasize (term:0.8): Deemphasize ((term)): Shorthand for (term:1.2) (((((((((term))))))))): Shorthand for (term:1.8) The following example shows how prompt weights contribute to the generated output.\nPrompt with weights ‚Äúeditorial product photo of (a translucent gel moisturizer jar:1.4) placed on a (frosted glass pedestal:1.2), surrounded by (dewy pink flower petals:1.1), with soft (diffused lighting:1.3), subtle water droplets, shallow depth of field‚Äù Prompt without weights ‚Äúeditorial product photo of a translucent gel moisturizer jar placed on a frosted glass pedestal, surrounded by dewy pink flower petals, with soft, subtle water droplets, shallow depth of field‚Äù You can also use weights in negative prompts to reduce how strongly the model avoids something. For example, ‚Äú(text:0.5), (blurry:0.2), (lowres:0.1).‚Äù This tells the model to be especially sure to avoid generating blurry text or low-resolution content.\nGiving specific stylistic guidance Effective prompt writing when using Stability AI Image Services such as Style Transfer and Style Guide requires a good understanding of style matching and reference-driven prompting. These techniques help provide clear stylistic direction for both text-to-image and image-to-image creation.\nImage-to-image style transfer extracts stylistic elements from an input image (control image) and uses it to guide the creation of an output image based on the prompt. Approach writing the prompt as if you‚Äôre directing a professional photographer or stylist. Focus on materials, lighting quality, and artistic intention‚Äînot just objects. For example, a well-structured prompt might read: ‚ÄúClose-up editorial photo of a translucent green lip gloss tube on crushed iridescent plastic, diffused colored lighting, shallow DOF, high fashion product styling.‚Äù\nStyle tag layering: Known aesthetic labels that align with brand identity The art of crafting effective prompts often relies on incorporating established style tags that resonate with familiar visual languages and datasets. By strategically blending terms from recognized aesthetic categories (ranging from editorial photography and analog film to anime, cyberpunk cityscapes, and brutalist structures), creators can guide the AI toward specific visual outcomes that align with their brand identity. These style descriptors serve as powerful anchors in the prompt engineering process. The versatility of these tags extends further through their ability to be combined and weighted, allowing for nuanced control over the final aesthetic. For instance, a skincare brand might blend the clean lines of product photography with dreamy, surreal elements, whereas a tech company could merge brutalist structure with cyberpunk elements for a distinctive visual identity. This approach to style mixing helps creators improve their outputs while maintaining clear ties to recognizable visual genres that resonate with their target audience. The key is understanding how these style tags interact and using their combinations to create unique, yet culturally relevant, visual expressions that serve specific creative or commercial objectives. The following table provides examples of prompts for a desired aesthetic.\nDesired aesthetic Prompt phrases Example use case Retro / Y2K 2000s nostalgia, flash photography, candy tones, harsh lighting Metallic textures, thin fonts, early digital feel. Clean modern neutral tones, soft gradients, minimalist styling, editorial layout Great for wellness or skincare products. Bold streetwear urban background, oversized fit, strong pose, midday shadow Fashion photography and lifestyle ads. Prioritize outfit structure and location cues. Hyperreal surrealism dreamcore lighting, glossy textures, cinematic DOF, surreal shadows Plays well in music, fashion, or alt-culture campaigns. Invoke a named style as a reference Some prompt structures benefit from invoking a named visual signature from a specific artist, especially when combined with your own stylistic phrasing or workflows, as shown in the following example.\nPrompt ‚Äúeditorial studio portrait of a woman with glowing skin in minimalist glam makeup, high-contrast lighting, clean background, (depiction of Van Gogh style:1.3)‚Äù The following is a more conceptual example.\nPrompt ‚Äúproduct shot of a silver hair oil bottle with soft reflections on curved chrome, (depiction of Wes Anderson style:1.2), under cold studio lighting‚Äù These phrases function like calling on a genre; they imply choices around materials, lighting, layout, and color tonality.\nUse reference images to guide style Another useful technique is using a reference image to guide the pose, color, or composition of the output. For use cases like matching a pose from a lookbook image, transferring a color palette from a campaign still, or copying shadowplay from a photo shoot, you can extract and apply structure or style from reference images.\nStability AI Image Services support a variety of image-to-image workflows where you can use a reference image (control image) to guide the output, such as Structure, Sketch, and Style. Tools like ControlNet (a neural network architecture developed by Stability AI that enhances control), IP-Adapter (an image prompt adapter), or clip-based captioning also enable further control when paired with Stability AI models.\nWe will discuss ControlNet, IP-Adapter, and clip-based captioning in a subsequent post.\nThe following is an example of an image-to-image workflow:\nFind a high-quality editorial reference. Use it with a depth, canny, or seg ControlNet to lock a pose. Style with a prompt. Prompt ‚Äúfashion editorial of a model in layered knitwear, dramatic colored lighting, strong shadows, high ISO texture‚Äù Create the right mood with lighting control In a prompt, lighting sets tone, adds dimensionality, and mimics the language of photography. It shouldn‚Äôt just be ‚Äúbright vs. dark.‚Äù Lighting is often the style itself, especially for audiences like Gen Z, for instance TikTok, early-aughts flash, harsh backlight, and color gels. The following table provides some useful lighting style prompt terms.\nLighting style Prompt terms Example use case High-contrast studio hard directional light, deep shadows, controlled highlights Beauty, tech, fashion with punchy visuals Soft editorial diffused light, soft shadows, ambient glow, overcast Skincare, fashion, wellness Colored gel lighting blue and pink gel lighting, dramatic color shadows, rim lighting Nightlife, music-adjacent fashion, youth-forward styling Natural bounce golden hour, soft natural light, sun flare, warm tones Outdoors, lifestyle, brand-friendly minimalism Build intent with posing and framing terms Good posing helps products feel aspirational and digital models more dynamic. With AI, you must be intentional. Framing and pose cues help avoid stiffness, anatomical errors, and randomness. The following table provides some useful posing and framing prompt terms.\nPrompt cue Description Tip looking off camera Creates candid or editorial energy Useful for lookbooks or ad pages hands in motion Adds realism and fluidity Avoids awkward, static body posture seated with body turned Adds depth and twist to the torso Reduces symmetry, feels natural shot from low angle Power or status cue Works well for stylized streetwear or product hero shots Example: Putting it all together The following example puts together what we‚Äôve discussed in this post.\nPrompt ‚Äústudio portrait of a model with platinum hair in metallic cargo pants and a cropped mesh hoodie, seated with legs wide on (acrylic stairs:1.6), magenta and teal gel lighting from left and behind, dramatic contrast, shot on 50mm, streetwear editorial for Gen Z campaign‚Äù Negative prompt ‚Äúblurry, extra limbs, watermark, cartoon, distorted face missing fingers, bad anatomy‚Äù Let‚Äôs break down the preceding prompt. We direct the look of the subject (platinum hair, metallic clothes), specify their pose (seated wide-legged, confident, unposed), define the environment (acrylic stairs and studio setup, controlled, modern), state the lighting (mixed gel sources, bold stylization), designate the lens (50mm, portrait realism), and lastly detail the purpose (for Gen Z campaign, sets visual and cultural tone). Together, the prompt produces the desired result.\nBest practices and troubleshooting Prompting is rarely a one-and-done task, especially for creative use cases. Most great images come from refining an idea over multiple attempts. Consider the following methodology to iterate over your prompts:\nKeep a prompt log Change one variable at a time Save seeds and base images Use comparison grids Sometimes things go wrong‚Äîmaybe the model ignores your prompt, or the image looks messy. These issues are common and often quick to fix, and you can get sharper, cleaner, and more intentional outputs with every adjustment. The following table provides useful tips for troubleshooting your prompts.\nProblem Cause of issue How to fix it Style feels random Model is confused or terms are vague Clarify style, add weight, remove conflicts Face gets warped Over-styled or lacks facial cues Add portrait of, headshot, or adjust pose or lighting Image is too dark Lighting not defined Add softbox from left, natural light, or time of day Repetitive poses Same seed or static structure Switch seed or change camera angle or subject action Lacks realism or feels ‚ÄúAI-ish‚Äù Wrong tone or artifacts Add negatives like cartoon, digital texture, distorted Conclusion Mastering advanced prompting techniques can turn basic image generation into professional creative outputs. Stability AI Image Services in Amazon Bedrock provide precise control over visual creation and editing, helping businesses convert concepts into production-ready assets. The combination of technical expertise and creative intent can help creators achieve the precision and consistency required in professional settings. This control proves valuable across multiple applications, such as marketing campaigns, brand consistency, and product visualizations. This post demonstrated how to optimize Stability AI Image Services in Amazon Bedrock to produce high-quality imagery that aligns with your creative goals.\nTo implement these techniques, access Stability AI Image Services through Amazon Bedrock or explore Stability AI‚Äôs foundation models available in Amazon SageMaker JumpStart. You can also find practical code examples in our GitHub repository.\nAbout the authors Maxfield Hulker is the VP of Community and Business Development at Stability AI. He is a longtime leader in the generative AI space. He has helped build creator-focused platforms like Civitai and Dream Studio. Maxfield regularly publishes guides and tutorials to make advanced AI techniques more accessible.\nSuleman Patel is a Senior Solutions Architect at Amazon Web Services (AWS), with a special focus on machine learning and modernization. Leveraging his expertise in both business and technology, Suleman helps customers design and build solutions that tackle real-world business problems. When he‚Äôs not immersed in his work, Suleman loves exploring the outdoors, taking road trips, and cooking up delicious dishes in the kitchen.\nIsha Dua is a Senior Solutions Architect based in the San Francisco Bay Area working with generative AI model providers and helping customer optimize their generative AI workloads on AWS. She helps enterprise customers grow by understanding their goals and challenges, and guides them on how they can architect their applications in a cloud-based manner while supporting resilience and scalability. She‚Äôs passionate about machine learning technologies and environmental sustainability.\nFabio Branco is a Senior Customer Solutions Manager at Amazon Web Services (AWS) and a strategic advisor, helping customers achieve business transformation, drive innovation through generative AI and data solutions, and successfully navigate their cloud journeys. Prior to AWS, he held Product Management, Engineering, Consulting, and Technology Delivery roles across multiple Fortune 500 companies in industries, including retail and consumer goods, oil and gas, financial services, insurance, and aerospace and defense.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AWS for Industries Connect to automotive or manufacturing plant displays using VNC and AWS IoT Secure Tunneling by Andrew Timpone, Jonathan Johnson, and Joseph O\u0026rsquo;Such on 18 SEP 2025 in Automotive, Industries, Internet of Things Permalink Comments Share\nIn the rapidly evolving automotive industry, remote access to vehicle displays has become a game-changer for maintenance, development, and support. By using VNC technology and AWS IoT Secure Tunneling, automotive professionals can now connect to remote vehicle displays for a wide array of applications. This capability enables remote diagnostics and troubleshooting, allowing technicians to securely access infotainment systems or instrument cluster displays of vehicles in the field, while engineers can remotely view and interact with advanced driver assistance system (ADAS) displays to help determine the need for debugging and tuning. During over-the-air (OTA) updates, engineers can monitor the progress and status screens remotely to confirm successful deployment. Fleet managers can use this technology to view driver information displays or telematics screens of multiple vehicles to help monitor performance, fuel efficiency, and improve driver behavior. In automotive manufacturing, quality control personnel can benefit from the ability to remotely access and inspect digital displays on vehicles moving through assembly lines. For vehicle testing and development, test engineers can remotely access prototype vehicle displays during road tests or simulations to help enhance safety and efficiency in testing procedures. Beyond these applications, the technology opens up possibilities for various other maintenance and support use cases, including onboard troubleshooting and guided co-browsing experiences, revolutionizing the way automotive professionals interact with and support vehicles in the field.\nOverview of solution This secure connectivity solution makes use of IoT Secure Tunneling to establish bidirectional communication to remote devices over a secure connection that is managed by AWS IoT. The solution uses a client machine with VNC Viewer installed, an intermediary server that acts as a secure local proxy, and an automotive display that you are connecting to.\nFigure 1: Architecture IoT Secure Tunneling components and messages\nPrerequisites For this walkthrough, you should have the following prerequisites:\nAn AWS account A VPC with a private subnet Basic familiarity with Linux commands and AWS IoT services A device to act as your IoT thing (destination) Recommend using a Raspberry Pi 4 VNC server installed and enabled AWS CLI installed and configured AWS IoT secure tunneling local proxy binary A server to act as a proxy relay (source) This can be an EC2 instance or ECS/EKS container AWS CLI installed and configured AWS IoT Secure Tunneling local proxy binary or container A client machine Recommend using either a Windows Bastion host in AWS or a personal computer VNC viewer installed Network connectivity to your proxy relay (source) Walkthrough Setup your Raspberry Pi The first step is to install and enable the VNC server, configure your Raspberry Pi as an IoT thing, and install or build the IoT Secure Tunneling local proxy.\nNote if the Pi does not recognize that it has a display to share, connect it to a monitor.\nInstall VNC server: sudo apt update sudo apt install realvnc-vnc-server Enable VNC server through raspi-config: sudo raspi-config Navigate to \u0026#34;Interface Options\u0026#34; \u0026gt; \u0026#34;VNC\u0026#34; \u0026gt; Enable Set VNC password: vncpasswd Configure your Raspberry Pi as an IoT thing Register your Raspberry Pi as an AWS IoT thing: aws iot create-thing --thing-name raspberry-pi-vnc Create required IoT policies and certificates (see detailed steps in setup instructions): aws iot create-policy \\ --policy-name SecureTunnelingPolicy \\ --policy-document \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iotsecuretunneling:RotateTunnelAccessToken\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iotsecuretunneling:SubscribeToTunnel\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iot:Connect\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iot:Subscribe\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] }\u0026#39; Follow the AWS IoT documentation to create and download certificates (here): aws iot create-keys-and-certificate \\ --set-as-active \\ --certificate-pem-outfile certificate_filename.pem \\ --public-key-outfile public_filename.key \\ --private-key-outfile private_filename.key Attach the policy to the certificate: aws iot attach-policy --policy-name SecureTunnelingPolicy --target \u0026lt;certificate-arn\u0026gt; Install and configure the local proxy (instructions can be found AWS IoT Secure Tunneling Local Proxy or in the Setup Local Proxy sections below) Create an IoT secure tunnel From the command line, issue the following AWS CLI command to your account and region where you are deploying your resources: aws iotsecuretunneling open-tunnel \\ --destination-config thingName=raspberry-pi-vnc,services=VNC \\ --region \u0026lt;your-aws-region\u0026gt; The command will return a JSON payload that contains the sourceAccessToken and destinationAccessToken (see documentation here). Note these down for later use. Best practice is to place the source and destination tokens into their own .txt files for easy use. Setup the local proxy using a binary file Within the private subnet, create an EC2 Ubuntu instance and follow the steps from AWS IoT Secure Tunneling Local Proxy to build the local proxy binary. Ensure that you can communicate from the Windows bastion host (described below) or personal device to the EC2 Ubuntu instance over TCP port 5000, and the instance has an IAM Role allowing Systems Manager access.\nConnect into the EC2 Ubuntu instance via AWS Session Manager or SSH. Run the following command from the command line from the directory that : the localproxy binary is located ./localproxy -s 5000 -b 0.0.0.0 -r \u0026lt;your-aws-region\u0026gt; -t \u0026lt;sourceAccessToken\u0026gt; On your Raspberry Pi, run the following command from the command to start the local proxy binary: ./localproxy -d 5000 -b 0.0.0.0 -r \u0026lt;your-aws-region\u0026gt; -t \u0026lt;destinationAccessToken\u0026gt; Setup the local proxy using a container Reference the Docker documentation to install docker before proceeding. The local proxy container can run on Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Service (ECS), if desired.\nTo download the container image and run on your EC2 instance, run the following command from the folder where your source token is stored sourceToken.txt. Replace \u0026lt;distro\u0026gt; with the appropriate distribution option for your machine from the list here:\ndocker run -d -it --network=host \\ public.ecr.aws/aws-iot-securetunneling-localproxy/\u0026lt;distro\u0026gt;-bin:amd64-latest \\ --region us-east-1 \\ -s 5000 \\ -c /etc/ssl/certs \\ -t $(cat sourceToken.txt) \\ -b 0.0.0.0 For raspberry Pi, run the following command where your destinationToken is stored destinationToken.txt. Replace \u0026lt;distro\u0026gt; with the appropriate distribution option for your machine from the list here:\ndocker run -d -it --network=host \\ public.ecr.aws/aws-iot-securetunneling-localproxy/\u0026lt;distro\u0026gt;-bin:amd64-latest \\ --region us-east-1 \\ -d 5000 \\ -c /etc/ssl/certs \\ -t $(cat destinationToken.txt) Setup the Windows bastion host with VNC Viewer Figure 2: VPC with bastion and Ubuntu local proxy\nIn order to experiment with IoT Secure Tunnel and the IoT LocalProxy, create a VPC with a public subnet containing a Windows bastion host, Internet Gateway, and NAT Gateway.\nIf you are going to use a personal device as the client machine, place your Ubuntu EC2 instance with LocalProxy in this public subnet instead.\nSetup the client machine On your client machine (either Windows Bastion host or personal computer), you will need to install a VNC viewer application. Follow the instructions here. If you are using a personal device, please ensure your EC2 proxy is in a public subnet.\nGet the IP address of the EC2 Ubuntu instance. If using your personal device, make sure this is the public IP address. Open the VNC Viewer application. Create a new connection. For the connection address, use the IP Address of the Ubuntu server and specify the port as 5000 (i.e. 172.31.59.116:5000) If you are using a personal computer, you will need to make sure your Ubuntu server has a public IP and is in a public subnet\nWhen prompted, enter the authentication information (i.e. username and password you setup on the Raspberry Pi for VNC) Cleaning up To help avoid incurring unnecessary future charges, stop using resources that are no longer needed:\nDelete the AWS IoT thing, certificates, and policies you created Close and delete any open tunnels using the AWS IoT console or CLI Stop the VNC server if no longer needed Terminate and delete any EC2 instances or ECS containers created Delete your VPC and VPC resources you created Conclusion In this post, we demonstrated how to establish secure user interface access to an IoT thing using AWS IoT Secure Tunneling and VNC. This solution can be adapted for various automotive, manufacturing, and industrial IoT scenarios where secure direct screen sharing access is crucial. Examples include remote diagnostics of vehicle infotainment systems and real-time monitoring of manufacturing equipment displays. We also shared details on leveraging built-in IoT messaging to automate the setup and teardown of the IoT secure tunnel and local proxy. Furthermore, this blog presented an architecture showcasing how to expand IoT secure tunnel automation with an AWS serverless architecture. This approach helps efficiently manage the creation and termination of IoT secure tunnels across a fleet of IoT things, whether they\u0026rsquo;re connected to vehicle or other displays. By implementing these solutions, automotive manufacturers and industrial operators can help significantly enhance their remote monitoring, troubleshooting, and maintenance capabilities, creating the potential for improved efficiency and reduced downtime.\nTo learn more, check out the AWS IoT Secure Tunneling documentation or explore other AWS IoT solutions.\nAbout the authors Andrew Timpone\nSix years of experience in cloud architecture, Andrew helps large enterprise customers solve their business problems using AWS. Andrew has over 25 years of IT experience with expertise in enterprise integration patterns. Andrew is married with three children and resides just south of Cleveland, OH, where he enjoys bicycle riding, archery, and vegetable gardening.\nJonathan Johnson\nJonathan Johnson (\u0026ldquo;JJ\u0026rdquo;) is a Senior Solutions Architect AWS. He has worked in the cloud architecture space for 10 years and 25+ years in IT \u0026amp; application development. He enjoys working with customers to design solutions for complex business requirements. More recently, JJ has been a champion of using Amazon Q Developer to build out solutions. When not designing AWS architectures, JJ can be found on his old Hunter sailboat \u0026lsquo;Seas the Day\u0026rsquo;.\nJoseph O\u0026rsquo;Such\nJoseph O\u0026rsquo;Such is a Partner Solutions Architect at AWS primarily supporting companies in the AWS Partner Network. Joseph often engages with IoT related projects and partners, stemming from his background in Mechanical engineering. Joseph lives and works in the Northern Virginia area outside of Washington DC, where he enjoys being active, including rock climbing, volleyball, and running.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Maximizing EC2 Spot Instance reliability for Nextflow on AWS Batch with Memory Machine Batch by Gabriela Karina Paulus, PhD and Christian Kniep on 18 SEP 2025 in Amazon EC2, AWS Batch, AWS Partner Network, Life Sciences, Nonprofit, Partner solutions, Public Sector, Public Sector Partners, Technical How-to Permalink Share\nAmazon Web Services (AWS) provides a comprehensive suite of services that enable life science organizations to run complex genomics workflows at scale. Through years of supporting cutting-edge research institutions, AWS has developed deep expertise in genomics computing, offering solutions that combine high performance with cost efficiency. At the heart of many genomics pipelines lies AWS Batch, a fully managed compute service that, when paired with Amazon Elastic Compute Cloud (Amazon EC2) Spot Instances, delivers powerful and cost-effective infrastructure for running Nextflow pipelines. By relying on this foundation, researchers can focus on scientific discovery rather than infrastructure management. To further enhance this robust platform, AWS partners with providers of innovative solutions such as MemVerge, an AWS Partner Network (APN) Advanced Technology Partner, whose Memory Machine Batch (MMBatch) technology complements AWS Batch by providing advanced checkpointing capabilities for EC2 Spot Instances. This collaboration exemplifies the AWS commitment to offering customers choice and flexibility in optimizing their research workflows.\nResearch institutes need to be on a constant lookout for how to reduce computational costs. Amazon EC2 Spot Instances are a great option for high-volume, noninteractive scientific workflows. With up to 90 percent cost savings compared to on-demand pricing, Spot Instances are attractive for high-throughput processing on a budget, but by default they are mainly suitable for fault-tolerant workloads because they can be reclaimed at any time if the asking price goes over the offered price.\nHowever, it‚Äôs important to understand that Spot Instance cost savings apply to compute time rather than workflow progress. When Spot Instances are reclaimed, running tasks are interrupted, which can result in lost computation time‚Äîpotentially hours of processing in long-running genomics workflows. While AWS Batch provides automated retry mechanisms for interrupted tasks, frequent Spot Instance reclamations can impact overall job completion times and make per-job costs less predictable. Organizations need to carefully evaluate their time-to-results requirements and implement appropriate strategies to manage these interruptions effectively.\nThis unpredictability has a lot of centers sticking to On-Demand Instances because they can‚Äôt risk incomplete analyses or missed deadlines. This experience isn‚Äôt unique. Many Nextflow users have encountered the challenges of working with Spot Instances when trying to balance cost-efficiency with reliability in cloud-based genomics workflows.\nIn this post, we look at how MemVerge‚Äôs technology can overcome the challenges of interrupted pipelines by enabling pipelines that were interrupted mid task to start from where they left off, regardless of Spot Instance reclaims. We‚Äôll also show how MemVerge‚Äôs Batch Viewer and proven best practices empower researchers to visualize their workloads, identify bottlenecks, and apply smart strategies that make their pipelines more efficient, resilient, and cloud-optimized.\nThe trade-off between cost and reliability in EC2 Spot Instances For research teams working under tight budget constraints, Spot Instances offer a compelling proposition‚Äîaccess to the same compute capacity as EC2 On-Demand Instances, but at a fraction of the cost. It‚Äôs a powerful way to stretch grant funding, accelerate time to discovery, and process more samples without compromising scientific goals.\nBut using Spot Instances comes with a catch.\nSpot Instances can be reclaimed by AWS with a 2-minute warning, depending on overall demand for capacity. In practice, this means that compute resources can vanish mid task, and long-running processes can fail without warning. Although Nextflow is designed to retry failed tasks, constant interruptions mean rerunning entire processes, increasing total execution time, and wasting already-consumed compute resources.\nEventually, centers shift critical stages of their workflows back to EC2 On-Demand Instances just to gain the reliability required to meet deadlines. While a shift works, it incurs steep cost‚Äîundoing the budgetary advantages that first motivated the move to the cloud.\nThis situation reveals a fundamental tension: Using Spot Instances has the potential to provide a steep discount for compute resources, but it doesn‚Äôt necessarily align with the requirements of the compute workflow that is orchestrating the processing power. For bioinformatics workloads that run for hours or days, that mismatch can be costly.\nSo how can research teams continue using EC2 Spot Instances without compromising reliability?\nWithin this ecosystem, MemVerge‚Äôs MMBatch serves as a complementary tool, adding transparent, container-level checkpointing to further enhance reliability.\nHow Nextflow helps‚Äîbut has limits Nextflow is designed to integrate seamlessly with cloud-native execution platforms, and when running on AWS, it hands off task execution to AWS Batch. This decouples workflow logic from the underlying compute layer, enabling AWS Batch to take full responsibility for managing retries, scaling resources, and reacting to Spot Instance reclaims.\nWithin AWS Batch retry strategies are used to control what happens to a job in certain circumstances. Common strategies are to retry the job when the host had an issue‚Äîfor example, the Spot instance got reclaimed by AWS due to a surge in demand from on-demand customers.\nWhen a Spot Instance is reclaimed, AWS Batch will requeue the job until the maximum number of job attempts are reached (up to 10). From Nextflow‚Äôs perspective, the task is simply retried‚Äîno manual intervention needed. This approach is elegant because it lets AWS Batch apply cloud-specific strategies and optimizations without burdening the workflow engine. It‚Äôs a smart division of labor: Nextflow defines the workflow and AWS Batch handles the execution.\nHowever, although this model improves reliability, it has a critical weakness: By default, every retry starts the job from scratch. The progress made in the previous attempt is usually lost.\nFor short tasks, that‚Äôs not a big deal. But for longer-running steps‚Äîsuch as aligning whole genomes or joint-calling across large sample sets‚Äîthe compute loss can be massive. A task that runs for 6 hours and gets interrupted in hour 5 will still need to restart from the beginning. If Spot Instance reclaims are frequent, the cost and delay multiply rapidly, undermining the budgetary advantage of using Spot Instances in the first place.\nThat‚Äôs where MemVerge‚Äôs MMBatch steps in to change the game.\nMMBatch doesn‚Äôt replace or override AWS Batch‚Äîit augments the way containers are launched within AWS Batch compute environments. Once integrated, MMBatch enables checkpointing of running jobs directly at the container level.\nWhen AWS Batch reschedules a job‚Äîsuch as after a Spot Instance reclaim‚ÄîMMBatch steps in during the container startup phase. Before the job begins executing again, MMBatch checks whether a checkpoint exists. If one is found, it automatically restores the container from that snapshot, picking up from the last saved state rather than starting over. From the AWS Batch perspective, nothing changes; it still sees a job being retried. But now, that retry isn‚Äôt a full do-over‚Äîit‚Äôs a fast recovery.\nThe result is powerful: The compute time already invested isn‚Äôt lost, even in the face of Spot Instance evictions. Pipelines become dramatically more resilient, long-running tasks are far less vulnerable, and the economics of Spot Instances become viable again‚Äîeven for workflows that used to require on-demand reliability.\nVisualizing and optimizing with MMBatch WebUI While checkpointing brings a huge leap in resilience, understanding and optimizing how pipelines behave in the cloud is just as important. That‚Äôs where the MMBatch Viewer comes in‚Äîa powerful UI that gives researchers and pipeline engineers real-time visibility into their workloads.\nFor many teams, cloud execution can feel like a black box. Tasks run remotely, on some instances, with variable costs and occasional failures. When things go wrong‚Äîor even just slower than expected‚Äîdebugging becomes time-consuming and often guesswork-driven.\nMMBatch WebUI changes that by making the execution layer transparent.\nWith MMBatch WebUI, users can:\nVisualize job timelines to see how long tasks actually take, where retries happen, and how Spot Instance reclaims affect progress. Track checkpoint activity, identifying which jobs benefit from snapshotting and how much compute is being saved. For example, a checkpoint interval of 15 minutes doesn‚Äôt checkpoint short running jobs and thus reduces the checkpoint pressure. Analyze cost and efficiency trends, helping teams pinpoint expensive or wasteful stages and refine their resource requests. Diagnose bottlenecks quickly, inspect log files or download a log bundle to open a support ticket. For the genomics research center, this visibility was a turning point. Instead of reacting to failed runs or spiraling costs, they could proactively tune their Nextflow configurations and AWS Batch strategies. They discovered which processes should be prioritized for checkpointing, adjusted memory and vCPU allocations based on real data, and even experimented with instance diversification strategies to reduce Spot Instance reclaim frequency.\nCombined with MemVerge‚Äôs checkpointing, MMBatch WebUI helped them transform Spot Instances from a risky gamble into a cost-efficient, high-throughput foundation for their sequencing work.\nMaking EC2 Spot Instances work for you Cloud-scale genomics doesn‚Äôt have to mean choosing between cost and reliability. Nextflow and AWS Batch provide a solid foundation‚Äîbut when EC2 Spot Instance reclaims threaten important deadlines, tools that go beyond retries are needed.\nWith MemVerge‚Äôs MMBatch, checkpointing makes even long-running jobs resilient to interruptions. And with the MMBatch WebUI, you gain the visibility and insight needed to tune your workflows for efficiency and cost-effectiveness.\nFor research teams running large-scale, time-sensitive workloads, these tools unlock the full potential of EC2 Spot Instances‚Äîwithout the tradeoffs.\nThe AWS advantage in research computing stems from years of experience supporting the world‚Äôs most demanding scientific workloads. With global infrastructure offering 99.99 percent availability, continuous innovation in high-performance computing, and deep expertise in genomics workflows, AWS provides the foundation for breakthrough research. Partner solutions like MemVerge‚Äôs MMBatch can enhance this foundation, in combination with comprehensive AWS offerings, security capabilities, and cost optimization at scale. As research organizations push the boundaries of scientific discovery, AWS offerings evolve to meet their growing needs, ensuring that technology never becomes a bottleneck to scientific progress.\nDiscover how your team can run faster, spend smarter, and hit every deadline‚Äîno matter how complex the science.\nIf you‚Äôre ready to dive deeper:\nWatch our walkthrough demo to see MemVerge in action. Try out our MMEngine Workshop on AWS Workshop Studio to gain hands-on experience. Talk to your MemVerge account team or visit MemVerge to schedule a personalized workshop. TAGS: Amazon EC2, AWS Batch, AWS Partner Network, AWS Public Sector, AWS Public Sector Partners, life sciences, nonprofit, technical how-to\nGabriela Karina Paulus, PhD Gabriela is a solutions architect at AWS, working within the NPO Sector and focusing on Research customers. She holds a PhD in Molecular Biology and Bioinformatics, a MSc. in Pharmacology, and a BSc. in Biotechnology. Based in the NYC Metropolitan Area, Gabriela is not only a solutions architect, but is also recognized as a genomics expert within AWS. She combines her scientific background with cloud computing expertise to drive innovation and to help scientists and research institutes perform their best work. Gabriela\u0026rsquo;s expertise in cloud technology is complemented by her extensive experience in research, with her previous peer-reviewed publication contributing valuable insights.\nChristian Kniep Christian‚Äôs passion is to improve the ease of use of those stubborn machines we call IT equipment. He started his career as an HPC Systems Engineer in the automotive industry and concluded the System admin job as the Infiniband lead of a crash-test cluster with thousands of nodes in the early 2010s. Afterwards he moved to Paris to work on HPC interconnects. Once Docker entered the picture in 2014, Christian moved back to Berlin and started working in (emerging) containerized environment at a Berlin-based Startup, Playstation Now, and Docker Inc. In 2019, Christian became an \u0026lsquo;EC2 spot specialist SA\u0026rsquo; at AWS and became the first senior developer advocate within the HPC product team. In 2022, Christian left AWS to once more pursue his passion project to push container usage in HPC. He worked for the Max Planck Institute for Human Development and joined MemVerge as principal HPC architect to foster the integration of application checkpoints and cloudy HPC stacks.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Le Anh Quan\nPhone Number: 0938347605\nEmail: nguyenleanhquan2005@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligent\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/5-workshop/5.1-workshop-overview/","title":"Module 1: Prerequisites","tags":[],"description":"","content":"Amazon QuickSight Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are. QuickSight connects to your data in the cloud and combines data from many different sources. In a single data dashboard, QuickSight can include AWS data, third-party data, big data, spreadsheet data, SaaS data, B2B data, and more. Key Features \u0026ldquo;SPICE: \u0026ldquo; The Super-fast, Parallel, In-memory Calculation Engine. SPICE is engineered to rapidly perform advanced calculations and serve data. \u0026ldquo;Auto-Graph\u0026rdquo; Automatically selects the best visualization for your data. \u0026ldquo;ML Insights: \u0026ldquo; Leverages machine learning to uncover hidden trends and outliers. \u0026ldquo;Embedded Analytics: \u0026ldquo; Embed dashboards into your applications. Workshop Scenario In this workshop, you will act as a Data Analyst for a retail company. You have been given a dataset of sales records and tasked with creating a dashboard to visualize sales performance by region, date, and product category.\nNext Steps Once you\u0026rsquo;ve completed these prerequisites, proceed to Module 2: Building Knowledge Base.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - IAM Fundamentals: + Create Users, Groups, and Roles + Write IAM Policies with Conditions + Set Permission Boundaries 09/09/2025 09/09/2025 AWS IAM Docs 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 10/09/2025 10/09/2025 https://000001.awsstudygroup.com// 5 - Cost Management: + Set up AWS Budgets and Alerts + Define Cost Allocation Tags + Request Quota increases via Service Quotas 11/09/2025 11/09/2025 AWS Cost Management 6 - Review \u0026amp; Practice: + Review IAM JSON Policy Structure + Lab: Configure Cross-Account Access using Roles 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups: Compute, Storage, Networking, Database, \u0026hellip; Successfully created and configured an AWS Free Tier account. Became familiar with the AWS Management Console and learned how to find, access, and use services via the web interface. Mastered IAM fundamentals including users, groups, roles, and policy creation with conditions. Successfully configured permission boundaries to limit maximum permissions for IAM entities. Learned AWS Single Sign-On (IAM Identity Center) for centralized identity management across multiple AWS accounts. Implemented resource tagging strategies for better organization and cost allocation. Set up AWS Budgets with alerts to monitor and control AWS spending. Defined cost allocation tags for detailed cost tracking and reporting. Learned to request quota increases via Service Quotas for scaling resources. Reviewed IAM JSON policy structure and syntax for writing custom policies. Successfully configured cross-account access using IAM roles for secure resource sharing. Notes and Knowledge Gained in Week 1: 1. Overview of Cloud Computing Definition: The delivery of IT resources on-demand over the internet, instead of purchasing and managing physical infrastructure yourself. Key Benefits: Pay-as-you-go: Only pay for what you use, helping optimize costs (e.g., shutting down servers at night). Accelerated Development: Automation features help deploy applications faster. Flexible Scalability (Elasticity): Easily increase/decrease resources (CPU, RAM) according to immediate needs. Global Deployment: Leverage provider infrastructure to serve users worldwide within minutes. 2. Real-world Market and Employer Requirements Vietnamese Market: Cloud workforce demand is very high in both technology companies and traditional industries (banking). Technology adoption is fast, no longer lagging behind the world. Employer Requirements: Hands-on Experience: Certifications are just the starting point. Most important is the ability to build and troubleshoot independently. Portfolio More Important than CV: Personal projects, technical blogs, Github source code are more valuable than a CV that just lists items. Foundational Knowledge: Must have solid knowledge of networking and operating systems. Soft Skills: Communication, teamwork, and English are essential factors. 3. Effective Learning Methods and Community Power Learning Methods: Long Journey: Requires persistence, cannot master in a few months, must continuously learn. Proactive Problem Solving: Develop the habit of researching independently before asking. Learn to \u0026ldquo;Build\u0026rdquo;: The ultimate goal is to create products and concrete solutions, not just follow existing labs. Don\u0026rsquo;t Fear Failure and Investment: Accept investing time, effort, and small costs for practice. Community Power: Supportive Environment: A place to learn and share knowledge without fear of judgment. Networking: Attending events is an opportunity to meet experts and potential employers. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: AWS Fundamentals \u0026amp; Core Services\nWeek 2: Compute Services - EC2 \u0026amp; Lambda\nWeek 3: Storage \u0026amp; Content Delivery\nWeek 4: Networking \u0026amp; VPC Configuration\nWeek 5: Security, Identity \u0026amp; Compliance\nWeek 6: Databases - RDS, DynamoDB \u0026amp; ElastiCache\nWeek 7: Application Integration \u0026amp; Messaging\nWeek 8: Monitoring, Logging \u0026amp; Cost Management\nWeek 9: DevOps \u0026amp; CI/CD Pipelines\nWeek 10: Containers \u0026amp; Orchestration\nWeek 11: Serverless Architecture \u0026amp; Advanced Services\nWeek 12: Final Project \u0026amp; Internship Review\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Document QA with AWS Bedrock Intelligent Document Analysis System using RAG 1. Executive Summary The Document QA system is a serverless application designed to revolutionize how users interact with documents. By leveraging AWS Bedrock for Generative AI and RAG (Retrieval Augmented Generation) technology, the platform allows users to upload PDF/TXT documents and ask natural language questions. The system provides accurate, context-aware answers by retrieving relevant information from the uploaded documents, significantly reducing manual search time and improving information accessibility.\n2. Problem Statement What\u0026rsquo;s the Problem? Traditional document search methods (keyword matching) often fail to capture context or semantic meaning. Manual document review is time-consuming, error-prone, and inefficient, especially for large volumes of text. Users struggle to extract specific insights quickly, leading to productivity bottlenecks.\nThe Solution We propose a Serverless RAG-based Chatbot using AWS Bedrock (Amazon Titan). The solution involves:\nUpload \u0026amp; Processing: Users upload documents to S3; Lambda functions trigger text extraction and embedding generation. Vector Search: Embeddings are stored and queried to find relevant document chunks. Generative AI: AWS Bedrock generates natural language responses based on the retrieved context. Serverless Architecture: Built on AWS Lambda, API Gateway, and DynamoDB for automatic scaling and cost efficiency. Benefits and Return on Investment Efficiency: Reduces document analysis time from hours to seconds. Accuracy: RAG ensures answers are grounded in the provided document, minimizing hallucinations. Cost-Effective: Serverless pay-as-you-go model (estimated \u0026lt; $5/month for low usage). Scalability: Automatically handles varying loads without manual infrastructure management. 3. Solution Architecture The platform employs a modern serverless architecture to ensure scalability, security, and performance.\nAWS Services Used AWS Bedrock: Provides the Foundation Models (Amazon Titan) for embeddings and text generation. AWS Lambda: Serverless compute for handling API requests, document processing, and orchestration. Amazon API Gateway: Manages REST API endpoints for the frontend. Amazon S3: Stores raw uploaded documents and frontend static assets. Amazon DynamoDB: Manages user sessions and chat history. Vector Store: (Implemented via Lambda/Local or dedicated vector DB) Stores document embeddings for semantic search. Component Design Frontend: Hosted on S3 (or Amplify), providing a user-friendly chat interface. API Layer: API Gateway routes requests (/upload, /ask) to Lambda functions. Processing Layer: Lambda handles text extraction, calls Bedrock for embeddings, and performs vector similarity search. AI Layer: AWS Bedrock generates responses using the retrieved context and user query. 4. Technical Implementation Implementation Phases\nPhase 1: Foundation (Weeks 1-4): Setup AWS environment, Bedrock access, and basic backend logic. Phase 2: API \u0026amp; Security (Weeks 5-7): Develop API Gateway, Lambda functions, and implement CORS/Security. Phase 3: Frontend Development (Weeks 8-11): Build the React/Next.js interface and integrate with APIs. Phase 4: Testing \u0026amp; Deployment (Weeks 12-14): End-to-end testing, optimization, and final deployment. Technical Requirements\nAI Model: Amazon Titan (via Bedrock) for Embeddings and Text Generation. Backend: Node.js/Python on AWS Lambda. Infrastructure as Code: Serverless Framework or AWS CDK. Frontend: React.js / Next.js. 5. Timeline \u0026amp; Milestones Month 1: Architecture Design, AWS Setup, Backend Core (Upload/Embeddings). Month 2: RAG Implementation, Vector Search Logic, API Development. Month 3: Frontend Integration, UI/UX Polish, Testing, and Launch. 6. Budget Estimation Estimated Monthly Costs (Low-Medium Usage)\nAWS Bedrock (Titan): ~$0 (Free Tier / Low cost per 1k tokens) AWS Lambda: ~$0.20 per 1M requests Amazon S3: ~$0.023 per GB Amazon DynamoDB: ~$0.25 per 1M requests Amazon API Gateway: ~$3.50 per 1M requests Total Estimated: \u0026lt; $5.00 / month\n7. Risk Assessment Risk Matrix Hallucinations (AI Errors): Medium Impact, Medium Probability. Cost Overruns: Medium Impact, Low Probability (Serverless). Data Leakage: High Impact, Low Probability. Mitigation Strategies Hallucinations: Strict RAG implementation (grounding answers in context). Cost: Set AWS Budget Alerts and usage quotas. Security: Use Presigned URLs for S3, IAM roles with least privilege. 8. Expected Outcomes Technical Improvements Fully automated document analysis pipeline. Sub-second retrieval latency for vector search. Scalable architecture supporting concurrent users. Long-term Value A reusable RAG framework for future knowledge base applications. Significant productivity gains for users needing quick information retrieval. 9. Team Structure and Responsibilities Name Student ID Primary Role Email/Contact Info Nguy·ªÖn L√™ Anh Qu√¢n SE192307 Team Leader/ Cloud Architect nguyenleanhquan2005@gmail.com ƒê√†o Quang Vinh SE180012 Project Manager/ Backend Developer (Bedrock, RAG) its.vnhdq@gmail.com Nguy·ªÖn Thanh Li√™m SE184163 Backend Developer liemntse184163@fpt.edu.vn Tr·∫ßn ƒê√¨nh Phong SE184217 Frontend Developer/ UI/UX Designer phongtdse184217@fpt.edu.vn D∆∞∆°ng Nguy·ªÖn Gia Huy SE182202 QA Engineer/Backend Developer (Bedrock, RAG) huydngse182202@fpt.edu.vn Detailed Responsibilities by Team Member Nguy·ªÖn L√™ Anh Qu√¢n - Cloud Architect/ Team Leader Primary Responsibilities:\nAWS architecture design and service selection Infrastructure planning and optimization Security architecture and IAM policies Technical consultation and best practices ƒê√†o Quang Vinh - Project Manager/Backend Developer Primary Responsibilities:\nOverall project management and timeline coordination Team coordination and task assignment Progress reporting to instructor/advisor Risk management and mitigation strategies Documentation oversight and quality assurance Build vector search and retrieval logic Develop chat/query handler Lambda function D∆∞∆°ng Nguy·ªÖn Gia Huy - QA Engineer/Backend Developer Primary Responsibilities:\nCore backend logic development Amazon Bedrock integration (Foundation Models) RAG (Retrieval-Augmented Generation) pipeline implementation Develop Lambda function for document ingestion Integrate Amazon Bedrock Knowledge Bases Implement text chunking and embedding generation Nguy·ªÖn Thanh Li√™m - Backend Developer Primary Responsibilities:\nBackend infrastructure and data management CI/CD pipeline development System monitoring and logging Performance optimization DynamoDB schema design and implementation Conversation history storage logic Tr·∫ßn ƒê√¨nh Phong - Frontend Developer Primary Responsibilities:\nUser interface design and development Frontend-backend integration User experience optimization Responsive design implementation Implement file upload interface with drag-and-drop Connect frontend to API Gateway endpoints Handle API responses and error states Deploy frontend to S3 + CloudFront "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/4-eventparticipated/4.3-event3/","title":"Event 2: AWS DevOps Full-Day Workshop","tags":[],"description":"","content":"AWS DevOps Workshop üìÖ Date: Monday, November 17, 2025\nüï£ Time: 8:30 AM ‚Äì 5:00 PM\nüìç Location: AWS Vietnam Office\nEvent Overview This comprehensive full-day workshop provided in-depth coverage of DevOps practices and AWS DevOps services. The event covered CI/CD pipelines, Infrastructure as Code, container services, and monitoring strategies, with hands-on demonstrations and real-world case studies.\nMorning Session (8:30 AM ‚Äì 12:00 PM) 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; DevOps Mindset Recap of AI/ML session from previous event DevOps culture and principles Benefits and key metrics: DORA (DevOps Research and Assessment) MTTR (Mean Time To Recovery) Deployment frequency 9:00 ‚Äì 10:30 AM | AWS DevOps Services ‚Äì CI/CD Pipeline Source Control:\nAWS CodeCommit Git strategies (GitFlow, Trunk-based development) Build \u0026amp; Test:\nCodeBuild configuration Testing pipelines Deployment:\nCodeDeploy strategies: Blue/Green deployments Canary releases Rolling updates Orchestration:\nCodePipeline automation Demo: Full CI/CD pipeline walkthrough\n10:30 ‚Äì 10:45 AM | Break 10:45 AM ‚Äì 12:00 PM | Infrastructure as Code (IaC) AWS CloudFormation:\nTemplates and syntax Stack management Drift detection AWS CDK (Cloud Development Kit):\nConstructs and patterns Reusable infrastructure components Multi-language support (TypeScript, Python, Java, C#) Demo: Deploying infrastructure with CloudFormation and CDK\nDiscussion: Choosing between IaC tools (CloudFormation vs CDK vs Terraform)\nLunch Break (12:00 ‚Äì 1:00 PM) Self-arranged lunch with networking opportunities\nAfternoon Session (1:00 ‚Äì 5:00 PM) 1:00 ‚Äì 2:30 PM | Container Services on AWS Docker Fundamentals:\nMicroservices architecture Containerization benefits Amazon ECR (Elastic Container Registry):\nImage storage and management Security scanning Lifecycle policies Amazon ECS \u0026amp; EKS:\nDeployment strategies Auto-scaling configurations Orchestration patterns AWS App Runner:\nSimplified container deployment Automatic scaling and load balancing Demo \u0026amp; Case Study: Microservices deployment comparison across ECS, EKS, and App Runner\n2:30 ‚Äì 2:45 PM | Break 2:45 ‚Äì 4:00 PM | Monitoring \u0026amp; Observability Amazon CloudWatch:\nMetrics collection and analysis Log aggregation and insights Alarms and notifications Custom dashboards AWS X-Ray:\nDistributed tracing Performance bottleneck identification Service map visualization Demo: Full-stack observability setup\nBest Practices:\nEffective alerting strategies Dashboard design principles On-call processes and incident response 4:00 ‚Äì 4:45 PM | DevOps Best Practices \u0026amp; Case Studies Deployment Strategies:\nFeature flags and progressive delivery A/B testing methodologies Automated Testing:\nUnit, integration, and end-to-end testing CI/CD integration patterns Incident Management:\nIncident response procedures Postmortem analysis Continuous improvement Case Studies:\nStartup DevOps transformations Enterprise-scale DevOps implementations Real-world success stories and lessons learned 4:45 ‚Äì 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up DevOps career pathways and opportunities AWS certification roadmap: AWS Certified DevOps Engineer ‚Äì Professional AWS Certified Solutions Architect AWS Certified Developer Key Takeaways Comprehensive understanding of DevOps culture and principles Hands-on experience with AWS CI/CD services (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) Practical knowledge of Infrastructure as Code using CloudFormation and CDK Deep dive into container orchestration with ECS, EKS, and App Runner Mastery of monitoring and observability tools (CloudWatch, X-Ray) Real-world deployment strategies and best practices Insights from enterprise DevOps case studies Skills Acquired CI/CD pipeline design and implementation Infrastructure as Code development Container orchestration and management Monitoring and observability setup Incident management and response DevOps metrics and KPI tracking Git workflow strategies Deployment automation techniques Event Photos "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Master VPCs, hybrid connectivity, and edge networking. Understand network monitoring, security, and content delivery. Learn advanced networking scenarios including VPC Peering, Transit Gateway, and Site-to-Site VPN. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - VPC Essentials: + Create VPC, Subnets (Public/Private) + Configure Route Tables and Internet Gateways + Workshop: Networking on AWS (Part 1) 15/09/2025 15/09/2025 AWS VPC Workshop 3 - Network Monitoring \u0026amp; Security: + Enable VPC Flow Logs + Configure AWS WAF (Web Application Firewall) + Set up VPC Endpoints (Gateway \u0026amp; Interface) 16/09/2025 16/09/2025 VPC Flow Logs\nAWS WAF\nVPC Endpoints 4 - Connectivity: + Configure VPC Peering + Explore AWS Transit Gateway + Manage DNS with Amazon Route 53 17/09/2025 17/09/2025 VPC Peering\nTransit Gateway\nRoute 53 Hybrid DNS 5 - Content Delivery: + Create CloudFront Distribution + Implement Lambda@Edge for dynamic routing + Configure Route 53 Geolocation Routing 18/09/2025 18/09/2025 https://000130.awsstudygroup.com/ 6 - Practice: Advanced VPC Scenarios + Simulate Site-to-Site VPN connection 19/09/2025 19/09/2025 Networking Workshop Week 2 Achievements: Successfully created VPCs with public and private subnets, route tables, and internet gateways. Configured VPC Flow Logs for network traffic monitoring and troubleshooting. Implemented AWS WAF to protect web applications from common exploits. Set up VPC Endpoints (Gateway and Interface) for private connectivity to AWS services. Configured VPC Peering for secure communication between VPCs. Explored AWS Transit Gateway for centralized network connectivity across multiple VPCs. Mastered Route 53 DNS management including geolocation routing policies. Created CloudFront distributions for global content delivery with low latency. Implemented Lambda@Edge for dynamic content routing at edge locations. Practiced advanced VPC scenarios and simulated Site-to-Site VPN connections. Notes and Knowledge Gained in Week 2: 1. VPC Essentials VPC Components: Subnets: Public subnets have routes to Internet Gateway, private subnets do not. Route Tables: Control traffic routing within VPC and to external networks. Internet Gateway: Enables communication between VPC and the internet. NAT Gateway: Allows private subnet instances to access internet without exposing them. CIDR Planning: Choose appropriate CIDR blocks to avoid IP conflicts with on-premises networks. Plan for future growth when allocating subnet ranges. Best Practices: Use private subnets for databases and backend servers. Place load balancers and bastion hosts in public subnets. Implement multiple availability zones for high availability. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/5-workshop/5.2-prerequiste/","title":"Module 2: Building Knowledge Base","tags":[],"description":"","content":"Building the Knowledge Base (Data Layer) Step 1: Sign up for Amazon QuickSight Log in to the AWS Management Console. Search for QuickSight. If you have not signed up yet, you will be prompted to do so. Choose Sign up for QuickSight. Select the Enterprise edition (it offers a free trial). Follow the on-screen instructions: Authentication method: Use IAM federated identity \u0026amp; QuickSight-managed users (default). Region: Select US East (N. Virginia). QuickSight account name: Enter a unique name. Notification email: Enter your email. Allow access: You can leave the defaults or uncheck them if we are only uploading a file. For this workshop, we don\u0026rsquo;t strictly need S3 access, but it\u0026rsquo;s good practice to leave S3 checked if you plan to use it later. Click Finish. Step 2: Prepare Sample Data We will use a simple CSV file for this workshop.\nCopy the following data and save it as a file named sales_data.csv on your computer: Date,Region,Product,Sales,Quantity 2023-01-01,North,Laptop,1200,2 2023-01-02,South,Phone,800,5 2023-01-03,East,Tablet,400,3 2023-01-04,West,Laptop,1200,1 2023-01-05,North,Phone,800,2 2023-01-06,South,Tablet,400,4 2023-01-07,East,Laptop,1200,3 2023-01-08,West,Phone,800,6 2023-01-09,North,Tablet,400,2 2023-01-10,South,Laptop,1200,4 Alternatively, you can use any CSV file you have, but the instructions will follow this structure.\nNext Steps Proceed to Module 3: Developing Serverless Backend to build Lambda functions that automate ingestion and handle chat queries.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Master AWS backup and storage services for data protection and hybrid cloud integration. Learn multi-account management and governance with AWS Organizations and Control Tower. Understand operations automation and infrastructure as code with Systems Manager and CloudFormation. Master core AWS services: EC2, IAM, RDS, Route 53, Auto Scaling, ELB, CloudFront, and S3. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - The team project topic has been selected: a chatbot using RAG and Bedrock. We are now planning the division of tasks for each project member - Learn AWS Backup for centralized backup management\n- Master AWS Storage Gateway for hybrid cloud storage\n- Understand Amazon S3 Glacier for archival storage\n- Learn AWS DataSync for automated data transfer\n- Practice:\n+ Configure backup plans and vaults\n+ Deploy File/Volume/Tape Gateway\n+ Implement Glacier lifecycle policies\n+ Set up DataSync tasks 09/29/2025 09/29/2025 AWS Backup\nStorage Gateway\nS3 Glacier\nDataSync 3 - Master AWS Control Tower for multi-account environment\n- Understand AWS Organizations for centralized management\n- Learn AWS Service Catalog for standardized provisioning\n- Practice:\n+ Set up Control Tower landing zone\n+ Configure OUs and guardrails\n+ Create Service Catalog portfolios 09/30/2025 09/30/2025 Control Tower\nOrganizations\nService Catalog 4 - Master AWS Systems Manager for operations management\n- Understand AWS CloudFormation for infrastructure as code\n- Practice:\n+ Configure Session Manager and Run Command\n+ Set up Patch Manager and Parameter Store\n+ Create CloudFormation stacks and StackSets 10/01/2025 10/01/2025 Systems Manager\nCloudFormation 5 - Learn Amazon EC2 fundamentals and compute services\n- Master AWS IAM for identity and access management\n- Understand Amazon RDS for managed databases\n- Learn Amazon Route 53 for DNS management\n- Practice:\n+ Launch EC2 instances with AMIs\n+ Configure IAM users, roles, and policies\n+ Deploy RDS Multi-AZ instances\n+ Set up Route 53 hosted zones 10/02/2025 10/02/2025 EC2\nIAM\nRDS\nRoute 53 6 - Master AWS Auto Scaling for capacity management\n- Understand Elastic Load Balancing for traffic distribution\n- Learn Amazon CloudFront for content delivery\n- Master Amazon S3 for object storage\n- Practice:\n+ Create Auto Scaling groups with policies\n+ Configure ALB and NLB\n+ Set up CloudFront distributions\n+ Implement S3 versioning and replication 10/03/2025 10/03/2025 Auto Scaling\nELB\nCloudFront\nS3 Week 3 Achievements: First step of my project with team, knowing what are we going to do. Successfully mastered 17 AWS services covering backup/storage, multi-account governance, operations automation, and core infrastructure. Configured AWS Backup with automated schedules, cross-region backup, and disaster recovery strategies. Deployed Storage Gateway for hybrid cloud integration with on-premises infrastructure. Implemented S3 Glacier lifecycle policies and Vault Lock for regulatory compliance. Established Control Tower landing zone with guardrails and Account Factory for multi-account management. Configured AWS Organizations with SCPs and consolidated billing for centralized governance. Mastered CloudFormation templates, nested stacks, and StackSets for infrastructure as code. Implemented Systems Manager for unified operations management including Session Manager, Run Command, and Patch Manager. Gained hands-on experience with EC2, IAM, RDS Multi-AZ, Route 53, Auto Scaling, ELB, CloudFront, and S3. Notes and Knowledge Gained in Week 3: Core AWS Services EC2: Compute service with various instance types, AMIs, security groups, EBS volumes, and CloudWatch monitoring. IAM: Identity and access management with users, groups, roles, policies, MFA, and least privilege principle. RDS: Managed relational databases with Multi-AZ for high availability, read replicas for scaling, automated backups, and encryption. Route 53: DNS service with hosted zones, routing policies (simple, weighted, latency, failover), health checks, and alias records. Auto Scaling: Automatic capacity management with scaling policies (target tracking, step, scheduled), lifecycle hooks, and CloudWatch integration. ELB: Traffic distribution with ALB (HTTP/HTTPS), NLB (TCP/UDP), target groups, health checks, and SSL/TLS termination. CloudFront: Global content delivery network with edge locations, cache behaviors, Lambda@Edge, origin failover, and signed URLs. S3: Object storage with storage classes, versioning, lifecycle policies, replication (CRR/SRR), encryption, and event notifications. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/5-workshop/5.3-connect-data/","title":"Module 3: Developing Serverless Backend","tags":[],"description":"","content":"CONNECT DATA Step 1: New Dataset In the QuickSight console, choose Datasets from the left navigation pane. Choose New dataset. Under FROM NEW DATA SOURCES, choose Upload a file. Select the sales_data.csv file you created in the previous step. Step 2: Preview and Edit After the file uploads, QuickSight will show a preview. Choose Edit settings and prepare data. Here you can see the columns: Date, Region, Product, Sales, Quantity. Ensure Sales and Quantity are detected as numbers (Integers or Decimals). Ensure Date is detected as a Date type. Choose Save \u0026amp; Publish at the top right. Choose Cancel to go back to the main screen, or directly click Visualize if prompted. Next Steps Proceed to Module 4: API \u0026amp; Security to expose these functions via API Gateway with authentication.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Prompting for precision with Stability AI Image Services in Amazon Bedrock This blog introduces advanced prompting techniques for Stability AI Image Services available in Amazon Bedrock to maximize image generation quality and precision. You will learn how to construct effective prompts using modular structures, apply prompt weighting to emphasize specific elements, and utilize negative prompts to ensure polished, professional outputs. The article provides a comprehensive guide on selecting the right prompt format‚Äînatural language, tag-based, or hybrid‚Äîand offers strategies for stylistic guidance using reference images and lighting controls. Additionally, it highlights best practices for troubleshooting and iterating on prompts to help enterprises create high-quality visual assets for marketing campaigns and product visualizations.\nBlog 2 - Connect to automotive or manufacturing plant displays using VNC and AWS IoT Secure Tunneling This blog introduces a secure connectivity solution for accessing remote vehicle and manufacturing displays using VNC technology and AWS IoT Secure Tunneling. You will learn how to establish bidirectional communication for critical use cases such as remote diagnostics, ADAS debugging, and monitoring over-the-air (OTA) updates. The article provides a comprehensive walkthrough on configuring the necessary architecture, including setting up a Raspberry Pi as an IoT thing, managing local proxies via binaries or containers, and using VNC Viewer for direct screen sharing. Additionally, it highlights how this architecture enhances maintenance capabilities and operational efficiency for automotive professionals and fleet managers.\nBlog 3 - Maximizing EC2 Spot Instance reliability for Nextflow on AWS Batch with Memory Machine Batch This blog introduces a solution for running cost-effective and resilient genomics workflows using Nextflow on AWS Batch. You will learn how to overcome the reliability challenges of Amazon EC2 Spot Instances by integrating MemVerge‚Äôs Memory Machine Batch (MMBatch). The article explains how MMBatch provides container-level checkpointing, allowing long-running jobs interrupted by Spot reclamations to resume from the last saved state instead of restarting from scratch. Additionally, it highlights how to use the MMBatch WebUI to visualize job timelines, optimize resource usage, and maximize the budget for life science research.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master S3, Block Storage, Backup strategies, and Hybrid storage. Understand data protection, encryption, and compliance. Learn storage performance optimization and cross-region replication. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Object Storage: + Host Static Website on S3 + Configure S3 Bucket Policies \u0026amp; Versioning + Implement Lifecycle Rules 29/09/2025 29/09/2025 https://000057.awsstudygroup.com/ 3 - Block Storage (EBS): + Automate Snapshots with Data Lifecycle Manager 30/09/2025 30/09/2025 Amazon EBS 4 - Block Storage (EBS): + Test EBS Multi-Attach + Review EBS Volume Types 01/10/2025 01/10/2025 https://000088.awsstudygroup.com/ 5 - Data Protection: + Configure AWS Backup Plans + Scan S3 with Amazon Macie + Create/Manage Keys in AWS KMS 02/10/2025 02/10/2025 AWS Backup 6 - Performance: + Complete Storage Performance Workshop + Practice: Setup S3 Cross-Region Replication 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Successfully hosted static websites on S3 with custom domain and HTTPS. Configured S3 bucket policies for secure access control and versioning for data protection. Implemented S3 lifecycle rules for automatic storage class transitions and cost optimization. Automated EBS snapshots using Data Lifecycle Manager for backup automation. Tested EBS Multi-Attach feature for shared block storage across multiple instances. Reviewed and selected appropriate EBS volume types based on performance requirements. Created Amazon FSx for Windows File Server for fully managed Windows file systems. Mounted Amazon EFS on Linux EC2 instances for shared file storage. Configured AWS Storage Gateway (File Gateway) for hybrid cloud storage integration. Set up AWS Backup plans for centralized backup management across AWS services. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software\nDate \u0026amp; Time: 14:00 ‚Äì 15:00, Friday, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 8:30 AM ‚Äì 12:00 PM, Saturday, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: 8:30 AM ‚Äì 5:00 PM, Monday, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/5-workshop/5.4-create-visuals/","title":"Module 4: API &amp; Security","tags":[],"description":"","content":"CREATE VISUALS Step 1: Create an Analysis If you are not already in the analysis view, go to Datasets, select your dataset, and choose Create analysis. You will see an empty workspace called a \u0026ldquo;Sheet\u0026rdquo;. Step 2: Visual 1 - Sales by Product (Donut Chart) In the Visual types pane (bottom left), select the Donut chart icon. From the Fields list (left), drag Product to the Group/Color well. Drag Sales to the Value well. You should see a donut chart showing sales distribution by product. Step 3: Visual 2 - Sales Trend over Time (Line Chart) Choose Add \u0026gt; Add visual from the top menu bar. Select the Line chart icon. Drag Date to the X axis well. Drag Sales to the Value well. You should see a line chart showing sales over time. Step 4: Visual 3 - Sales by Region (Bar Chart) Choose Add \u0026gt; Add visual. Select the Vertical bar chart icon. Drag Region to the X axis well. Drag Sales to the Value well. You can now see which region performs best. Next Steps Proceed to Module 5: Frontend Integration \u0026amp; Testing to build and deploy the React application.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master Infrastructure as Code using CloudFormation and AWS CDK. Automate infrastructure deployment and management. Learn best practices for IaC and static site deployment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - CloudFormation: + Write CloudFormation Template (YAML) + Deploy VPC Stack + Handle Stack Updates and Drift 06/10/2025 06/10/2025 CloudFormation Workshop 3 - AWS CDK Essentials: + Install CDK and Initialize Project + Write Infrastructure in Python/TypeScript + Practice: Deploy EC2 instance via CDK 07/10/2025 07/10/2025 AWS CDK 4 - Advanced Automation: + Explore CDK Constructs for ECS + Review EKS Blueprints + Manage State Files 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Static Sites CI/CD: + Deploy Static Site using Amplify Console + Architect WordPress High Availability + Review WordPress on EC2 patterns 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Review: + Refactor Week 2 Networking Lab using CDK + Read \u0026ldquo;Infrastructure as Code\u0026rdquo; Best Practices 10/10/2025 10/10/2025 AWS Well-Architected Week 5 Achievements: Successfully wrote CloudFormation templates in YAML for infrastructure automation. Deployed VPC stacks using CloudFormation with proper resource dependencies. Handled stack updates and detected configuration drift. Installed and configured AWS CDK for infrastructure as code. Wrote infrastructure code in Python/TypeScript using CDK constructs. Deployed EC2 instances programmatically via CDK. Explored advanced CDK constructs for ECS and EKS. Deployed static sites using AWS Amplify Console with CI/CD. Architected high availability WordPress solutions on AWS. Refactored networking infrastructure using CDK best practices. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/5-workshop/5.5-public-dashboard/","title":"Module 5: Frontend Integration &amp; Testing","tags":[],"description":"","content":"PUBLISH DASHBOARD Step 1: Arrange Visuals You can resize and move the visuals on the sheet to make them look organized. Click on the title of each visual to rename it (e.g., \u0026ldquo;Sales by Product\u0026rdquo;). Add a title to the sheet at the top (e.g., \u0026ldquo;Sales Overview Dashboard\u0026rdquo;). Step 2: Publish Dashboard At the top right, choose Share \u0026gt; Publish dashboard. Publish new dashboard as: Enter a name, e.g., Sales-Dashboard-v1. Choose Publish dashboard. You can now share this dashboard with other users in your QuickSight account or share it via a link. Congratulations! You have successfully created a Business Intelligence dashboard using Amazon QuickSight.\nNext Steps Proceed to Module 6: Clean Up Resources to avoid ongoing charges.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Workshop Business Intelligence with Amazon QuickSight Overview In this workshop, you will learn how to use Amazon QuickSight, a fast, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization.\nYou will sign up for Amazon QuickSight, connect to a data source, create an analysis with various visualizations, and finally publish a dashboard.\nContent Workshop overview Prerequiste Connect Data Create Visuals Publish Dashboard Clean up "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master relational and NoSQL databases on AWS. Learn caching strategies with ElastiCache. Understand database migration and disaster recovery. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Relational DB: + Launch RDS Instance (MySQL/PostgreSQL) + Store Credentials in Secrets Manager + Configure Multi-AZ and Read Replicas 13/10/2025 13/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - NoSQL \u0026amp; Caching: + Create DynamoDB Tables + Write Items via CLI/SDK + Launch ElastiCache (Redis) Cluster 14/10/2025 14/10/2025 DynamoDB Developer Guide 4 - Advanced DB: + Complete PostgreSQL Workshop (Part 1) + Explore SQL Server HA on AWS + Understand RDS Proxy 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Migration Tools: + Study Database Migration Service (DMS) + Understand Schema Conversion Tool (SCT) + Review Server Migration Service 16/10/2025 16/10/2025 AWS DMS 6 - Disaster Recovery: + Configure AWS Elastic Disaster Recovery (DRS) + Practice: Failover RDS to Standby Region 17/10/2025 17/10/2025 AWS DRS Week 6 Achievements: Successfully launched RDS instances with MySQL and PostgreSQL engines. Stored database credentials securely in AWS Secrets Manager. Configured Multi-AZ deployments for high availability. Set up read replicas for read scaling and disaster recovery. Created DynamoDB tables with partition and sort keys. Performed CRUD operations on DynamoDB via CLI and SDK. Launched ElastiCache Redis clusters for application caching. Completed PostgreSQL workshop and learned advanced features. Explored SQL Server high availability patterns on AWS. Understood RDS Proxy for connection pooling and failover. Studied AWS DMS for database migration with minimal downtime. Learned Schema Conversion Tool for heterogeneous migrations. Configured AWS Elastic Disaster Recovery for application resilience. Practiced RDS failover to standby regions. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/5-workshop/5.6-cleanup/","title":"Module 6: Clean Up Resources","tags":[],"description":"","content":"CLEAN UP Clean up resources To avoid incurring future charges, you should unsubscribe from QuickSight if you do not plan to use it further.\nManage QuickSight:\nClick on your user icon in the top right corner. Choose Manage QuickSight. Unsubscribe from QuickSight:\nGo to Account settings. Choose Delete account (this unsubscribes you from QuickSight). Type delete to confirm. Choose Delete account. Note: If you are on the Free Trial, you will not be charged until the trial ends, but it is good practice to clean up if you are done.\n"},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Amazon Web Services Vietnam Co., Ltd] from [12/08/2025] to [12/12/2025], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [RAG chatbot project], through which I improved my skills in [programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚òê ‚úÖ 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚òê ‚úÖ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚òê ‚úÖ ‚òê 12 Overall General evaluation of the entire internship period ‚òê ‚úÖ ‚òê Needs Improvement Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively Improve time management skills to be better and more disciplined; I tend to procrastinate until the last minute. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Master CloudWatch for monitoring and observability. Learn scaling strategies and capacity planning. Understand security monitoring and incident response. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - CloudWatch Core: + Create CloudWatch Dashboards + Configure CloudWatch Alarms + Workshop: CloudWatch Advanced 20/10/2025 20/10/2025 CloudWatch Workshop 3 - Scaling: + Configure EC2 Auto Scaling Groups + Define Dynamic Scaling Policies + Analyze Reserved Capacity needs 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Advanced Security Monitoring: + Enable AWS GuardDuty + Explore AWS Security Hub findings + Configure Firewall Manager policies 22/10/2025 22/10/2025 AWS Security Hub 5 - Operations: + Automate Ops with Lambda + Instrument App with X-Ray Tracing + Analyze Logs Insights 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Integration: + Build Operations Dashboard (EC2/RDS) + Configure SNS topics for Alerts 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Created comprehensive CloudWatch dashboards for infrastructure monitoring. Configured CloudWatch alarms with SNS notifications for proactive alerting. Completed advanced CloudWatch workshop for observability best practices. Configured EC2 Auto Scaling groups with target tracking policies. Defined dynamic scaling policies based on CloudWatch metrics. Analyzed Reserved Instance and Savings Plans for cost optimization. Enabled AWS GuardDuty for intelligent threat detection. Explored AWS Security Hub for centralized security findings. Configured AWS Firewall Manager for centralized firewall policies. Automated operational tasks using Lambda functions. Instrumented applications with AWS X-Ray for distributed tracing. Analyzed application logs using CloudWatch Logs Insights. Built unified operations dashboard for EC2 and RDS monitoring. Configured SNS topics for multi-channel alert notifications. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/7-feedback/","title":"Feedback &amp; Suggestions","tags":[],"description":"","content":"Here you can freely share your personal opinions about your experiences participating in the First Cloud Journey program, helping the FCJ team improve any shortcomings based on the following categories:\nGeneral Evaluation 1. Work Environment\nThe work environment is very friendly and open. FCJ members are always ready to help when I encounter difficulties, even outside working hours. The workspace is neat and comfortable, helping me focus better. However, the constantly changing environment is also a downside for me. Having a fixed number of sessions registered at the beginning of each week would make time management easier.\n2. Mentor / Admin Team Support\nThe mentor provides very detailed guidance, explains clearly when I don\u0026rsquo;t understand, and always encourages me to ask questions. The admin team supports procedures, documentation, and creates favorable conditions for me to work smoothly. I highly appreciate that the mentor allows me to try and handle problems myself rather than just giving answers.\n3. Alignment Between Work and Major\nThe work I was assigned was not quite aligned with the knowledge I learned at school, while also expanding into new areas that were somewhat challenging for me. Thanks to this, I both learned additional practical skills and new skills, while also preparing myself for the future. Learning new things is never redundant.\n4. Learning Opportunities \u0026amp; Skill Development\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared many practical experiences that helped me better orient my career.\n5. Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still has fun. When there are urgent projects, everyone works together and supports each other regardless of position. This makes me feel like I\u0026rsquo;m part of the team, even though I\u0026rsquo;m just an intern. Because the office schedule is not fixed, I couldn\u0026rsquo;t get to know new friends, except for those in my group.\n6. Policies / Benefits for Interns\nThe company provides internship allowances and creates flexible time conditions when necessary. Additionally, being able to participate in internal training sessions is a big plus.\nOther Questions What are you most satisfied with during your internship?\nHelping me understand more about cloud. This will be beneficial for my career path in the future. What do you think the company needs to improve for future interns?\nI hope future interns will get to work on exercises that are increasingly closer to reality and more aligned with the market. Instead of just doing free projects. If recommending to friends, would you advise them to intern here? Why?\nI\u0026rsquo;m not sure, the environment here is positive but perhaps not suitable for me, but it may be suitable for others. Suggestions \u0026amp; Wishes Do you have any suggestions to improve the internship experience?\nNone Would you like to continue this program in the future?\nNone Other feedback (feel free to share):\nNone "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Master serverless architecture with Lambda and API Gateway. Learn event-driven patterns and messaging services. Build and deploy a complete serverless application (Project 1: Book Store). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Serverless Basics: + Learn Lambda \u0026amp; API Gateway integration + Study Microservices Architecture + Explore Event-Driven Patterns 27/10/2025 27/10/2025 Serverless Land 3 - Messaging: + Setup SQS Queues and SNS Topics + Design Workflow with Step Functions + Trigger Lambda from SQS 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Project 1 (Book Store): + Build Backend (Lambda + DynamoDB) + Setup API Gateway (REST/HTTP) + Implement Cognito Authentication 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Project 1 (Continued): + Integrate Frontend with API + Configure Custom Domain \u0026amp; SSL + Test End-to-End flow 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Project 1 (Finalize): + Automate Deployment with AWS SAM + Create CI/CD for Serverless App 31/10/2025 31/10/2025 AWS SAM Week 8 Achievements: Mastered AWS Lambda function development and deployment. Integrated Lambda with API Gateway for RESTful APIs. Studied microservices architecture patterns and best practices. Explored event-driven architecture with AWS services. Set up SQS queues for asynchronous message processing. Configured SNS topics for pub/sub messaging patterns. Designed complex workflows using AWS Step Functions. Implemented Lambda triggers from SQS for event processing. Built complete serverless backend with Lambda and DynamoDB. Created REST and HTTP APIs using API Gateway. Implemented user authentication with Amazon Cognito. Integrated frontend application with serverless backend. Configured custom domains and SSL certificates for APIs. Tested end-to-end application flow and error handling. Automated serverless deployment using AWS SAM. Created CI/CD pipeline for serverless applications. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Master DevOps practices with AWS CI/CD services. Learn Elastic Beanstalk for application deployment. Build and deploy a document management system (Project 2). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - CI/CD Fundamentals: + Create Repo in CodeCommit/GitHub + Build pipeline with CodePipeline + Automate Build with CodeBuild 03/11/2025 03/11/2025 AWS DevOps 3 - Elastic Beanstalk: + Deploy Node.js App to Beanstalk + Configure Environment Properties + Manage Rolling Updates 04/11/2025 04/11/2025 Elastic Beanstalk 4 - Project 2 (Doc Management): + Build CRUD API with Lambda + Integrate AWS Amplify for Auth/Storage + Connect Frontend to API Gateway 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Project 2 (Continued): + Setup CloudFront for Content Delivery + Implement Search (OpenSearch/CloudSearch) + Configure CI/CD pipeline 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Wrap up: + Complete Serverless Web App Workshop + Build simple Serverless Chat App 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Created Git repositories in CodeCommit and GitHub for version control. Built automated CI/CD pipelines using AWS CodePipeline. Configured CodeBuild for automated application builds and tests. Deployed Node.js applications to Elastic Beanstalk. Configured environment properties and managed application versions. Implemented rolling updates for zero-downtime deployments. Built CRUD API with Lambda for document management. Integrated AWS Amplify for authentication and file storage. Connected frontend application to API Gateway endpoints. Set up CloudFront distribution for global content delivery. Implemented search functionality using Amazon OpenSearch. Configured end-to-end CI/CD pipeline for the project. Completed serverless web application workshop. Built real-time serverless chat application. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Master container technologies with Docker, ECS, and EKS. Learn container orchestration and deployment strategies. Implement CI/CD pipelines for containerized applications. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Container Basics: + Dockerize a Sample Application + Push Image to Amazon ECR + Deploy to Lightsail Containers 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Amazon ECS: + Create ECS Cluster (Fargate) + Define Task Definitions \u0026amp; Services + Workshop: Amazon ECS 11/11/2025 11/11/2025 ECS Workshop 4 - ECS DevOps: + Build CI/CD for ECS + Write ECS Infrastructure using CDK + Implement Blue/Green Deployment 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Amazon EKS: + Create EKS Cluster + Deploy App using kubectl + Workshop: Amazon EKS 13/11/2025 13/11/2025 EKS Workshop 6 - Advanced Containers: + Setup CI/CD for EKS + Overview of Red Hat OpenShift (ROSA) 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Successfully dockerized sample applications with multi-stage builds. Pushed container images to Amazon ECR with automated scanning. Deployed containerized applications to Lightsail Containers. Created ECS clusters using Fargate for serverless containers. Defined ECS task definitions with resource limits and health checks. Configured ECS services with load balancing and auto-scaling. Completed comprehensive Amazon ECS workshop. Built CI/CD pipelines for automated ECS deployments. Wrote ECS infrastructure as code using AWS CDK. Implemented blue/green deployment strategy for zero-downtime updates. Created production-ready EKS clusters with managed node groups. Deployed applications to EKS using kubectl and Helm. Completed Amazon EKS workshop with hands-on labs. Set up CI/CD pipelines for Kubernetes applications. Explored Red Hat OpenShift on AWS (ROSA) features. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Master data analytics and business intelligence on AWS. Learn data lake architecture and data processing. Build dashboards and visualizations with QuickSight. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Data Lake: + Learn Data Lake Concepts on S3 + Build a Data Lake with sample data + Secure Data Lake permissions 17/11/2025 17/11/2025 AWS Data Lakes 3 - Data Processing: + Crawl data using AWS Glue + Query S3 data using Amazon Athena + Analyze Cost Reports via Athena 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Business Intelligence: + Connect QuickSight to Athena + Create Visualizations \u0026amp; Dashboards + Share Insights 19/11/2025 19/11/2025 Amazon QuickSight 5 - Data Engineering: + Participate in Data Engineering Immersion Day (Labs) + Explore AWS AI Services 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Review: + Practice: Query VPC Flow Logs with Athena + Build Cost Dashboard in QuickSight 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Learned data lake architecture concepts and best practices. Built data lake on S3 with organized folder structure. Implemented data lake security with IAM policies and Lake Formation. Crawled data sources using AWS Glue crawlers for metadata discovery. Queried S3 data using Amazon Athena with SQL. Analyzed AWS Cost and Usage Reports with Athena queries. Connected Amazon QuickSight to Athena data sources. Created interactive visualizations and dashboards in QuickSight. Shared dashboards with stakeholders and configured permissions. Participated in Data Engineering Immersion Day workshops. Explored AWS AI services (Rekognition, Comprehend, Translate). Queried VPC Flow Logs with Athena for network analysis. Built comprehensive cost optimization dashboard in QuickSight. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Master high availability architecture design. Learn FinOps and cost optimization strategies. Build and test a resilient capstone project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - High Availability: + Design 3-Tier HA Architecture + Plan Multi-AZ Database Strategy + Review Load Balancing patterns 24/11/2025 24/11/2025 AWS Architecture Center 3 - FinOps \u0026amp; Cost: + Analyze Savings Plans \u0026amp; Reserved Instances + Review AWS Compute Optimizer + Implement Cost Optimization strategies 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Capstone Implementation: + Deploy HA Stack (VPC, ASG, ALB, RDS) + Apply Security Groups and WAF + Configure Monitoring 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Chaos Engineering: + Simulate Component Failures (Fault Injection) + Validate Auto Recovery + Test Backup Restoration 27/11/2025 27/11/2025 AWS Fault Injection 6 - Final Review: + Document Capstone Architecture + Clean up Resources + Prepare Final Presentation 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Designed comprehensive 3-tier high availability architecture. Planned Multi-AZ database strategy with automatic failover. Reviewed and implemented load balancing patterns for resilience. Analyzed Savings Plans and Reserved Instances for cost optimization. Used AWS Compute Optimizer for right-sizing recommendations. Implemented cost optimization strategies across all services. Deployed complete HA stack with VPC, Auto Scaling, ALB, and RDS. Applied security best practices with Security Groups and WAF. Configured comprehensive monitoring with CloudWatch and alarms. Performed chaos engineering experiments with AWS FIS. Simulated component failures and validated auto-recovery mechanisms. Tested backup and restoration procedures for disaster recovery. Documented complete capstone architecture with diagrams. Cleaned up all AWS resources to avoid unnecessary costs. Prepared final presentation showcasing 12-week journey. "},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://nguyenleanhquan2005.github.io/AWS_Report/tags/","title":"Tags","tags":[],"description":"","content":""}]